{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LABo 6 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Simple Linear Regression\n",
    "\n",
    "### Step 1: Train a Linear Regression Model\n",
    "\n",
    "* Load the Alzheimer dataset using `AlzheimerDataset`.\n",
    "* Extract the target variable (`ADL`) and the predictor variables (`MMSE` and `FunctionalAssessment`).\n",
    "* Train a `LinearRegression` model on the training data.\n",
    "\n",
    "### Step 2: Cross-validation and Model Comparison\n",
    "\n",
    "* Perform cross-validation on both `LinearRegression` and `Ridge` models.\n",
    "* Evaluate the performance of each model using appropriate metrics (e.g., R-squared, Mean Squared Error).\n",
    "* Compare the cross-validation scores of the two models.\n",
    "\n",
    "### Step 3: Analyze Coefficients\n",
    "\n",
    "* Examine the coefficients of the trained `LinearRegression` model.\n",
    "* Determine the influence of each predictor variable (`MMSE` and `FunctionalAssessment`) on the target variable (`ADL`).\n",
    "\n",
    "### Step 4: Test Set Evaluation and Comparison\n",
    "\n",
    "* Evaluate the performance of both `LinearRegression` and `Ridge` models on the held-out test set using the Root Mean Squared Error (RMSE) metric.\n",
    "* Conduct a statistical test (e.g., paired t-test) to compare the RMSE values of the two models and determine if the difference is statistically significant.\n",
    "\n",
    "**Note:**\n",
    "\n",
    "* This exercise focuses on building and evaluating simple linear regression models.\n",
    "* Pay close attention to data preprocessing and feature scaling when working with regression models.\n",
    "* Remember to avoid excessive reliance on generative AI tools to enhance your learning experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Results:\n",
      "Coefficients: [0.07668555 0.14679183]\n",
      "Intercept: 2.3707350510966405\n",
      "Mean Squared Error: 8.8464\n",
      "R2 Score: 0.0237\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "from alzheimer import AlzheimerDataset\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "# Définir _CWD comme le répertoire de travail actuel\n",
    "_CWD = pathlib.Path(os.getcwd())\n",
    "\n",
    "# Helper function to prepare data\n",
    "def get_selected_vars(data, X_vars, Y_var):\n",
    "    X = data.numericals[X_vars].to_numpy()\n",
    "    Y = data.numericals[Y_var].to_numpy()\n",
    "    return X, Y\n",
    "\n",
    "# Main function for Exercise 1.1\n",
    "def main():\n",
    "    # Load the Alzheimer dataset\n",
    "    dataset = AlzheimerDataset(\"C:\\\\Users\\\\andre\\\\Data-20241216\\\\alzheimers_disease_data.csv\")\n",
    "\n",
    "    train_data, test_data = dataset.random_split(test_ratio=0.2, random_state=0)\n",
    "\n",
    "    # Define input (X) and target (Y) variables\n",
    "    X_vars = ['MMSE', 'FunctionalAssessment']\n",
    "    Y_var = 'ADL'\n",
    "    \n",
    "    # Get training and testing data\n",
    "    X_train, Y_train = get_selected_vars(train_data, X_vars, Y_var)\n",
    "    X_test, Y_test = get_selected_vars(test_data, X_vars, Y_var)\n",
    "\n",
    "    # Initialize and train the LinearRegression model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    # Predict on the test set\n",
    "    Y_pred = model.predict(X_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    mse = mean_squared_error(Y_test, Y_pred)\n",
    "    r2 = r2_score(Y_test, Y_pred)\n",
    "\n",
    "    # Print results\n",
    "    print(\"Linear Regression Results:\")\n",
    "    print(f\"Coefficients: {model.coef_}\")\n",
    "    print(f\"Intercept: {model.intercept_}\")\n",
    "    print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "    print(f\"R2 Score: {r2:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Results:\n",
      "Coefficients: [0.07668555 0.14679183]\n",
      "Intercept: 2.3707350510966405\n",
      "Mean Squared Error: 8.8464\n",
      "R2 Score: 0.0237\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "from alzheimer import AlzheimerDataset\n",
    "import pathlib\n",
    "import os\n",
    "\n",
    "# Define working directory\n",
    "_CWD = pathlib.Path(os.getcwd())\n",
    "\n",
    "def get_selected_vars(data, X_vars, Y_var):\n",
    "    \"\"\"Extract selected variables.\"\"\"\n",
    "    X = data.numericals[X_vars].to_numpy()\n",
    "    Y = data.numericals[Y_var].to_numpy()\n",
    "    return X, Y\n",
    "\n",
    "def main():\n",
    "    # Load the dataset\n",
    "    dataset = AlzheimerDataset(\"C:\\\\Users\\\\andre\\\\Data-20241216\\\\alzheimers_disease_data.csv\")\n",
    "\n",
    "    \n",
    "    # Split into training and test sets\n",
    "    train_data, test_data = dataset.random_split(test_ratio=0.2, random_state=0)\n",
    "    \n",
    "    # Select variables\n",
    "    X_vars = ['MMSE', 'FunctionalAssessment']\n",
    "    Y_var = 'ADL'\n",
    "    X_train, Y_train = get_selected_vars(train_data, X_vars, Y_var)\n",
    "    X_test, Y_test = get_selected_vars(test_data, X_vars, Y_var)\n",
    "    \n",
    "    # Train Linear Regression model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, Y_train)\n",
    "    \n",
    "    # Print coefficients and intercept\n",
    "    print(\"Linear Regression Results:\")\n",
    "    print(f\"Coefficients: {model.coef_}\")\n",
    "    print(f\"Intercept: {model.intercept_}\")\n",
    "    \n",
    "    # Evaluate on the test set\n",
    "    Y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(Y_test, Y_pred)\n",
    "    r2 = r2_score(Y_test, Y_pred)\n",
    "    \n",
    "    print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "    print(f\"R2 Score: {r2:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression - Average RMSE: 2.8288\n",
      "Ridge Regression - Average RMSE: 2.8285\n",
      "Ridge Regression performs better on average.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andre\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "c:\\Users\\andre\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "c:\\Users\\andre\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "c:\\Users\\andre\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "c:\\Users\\andre\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "c:\\Users\\andre\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "c:\\Users\\andre\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "c:\\Users\\andre\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "c:\\Users\\andre\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "c:\\Users\\andre\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "import numpy as np\n",
    "from alzheimer import AlzheimerDataset\n",
    "\n",
    "# Load the dataset\n",
    "file_path = r\"C:\\Users\\andre\\Downloads\\Data-20241216\\alzheimers_disease_data.csv\"\n",
    "dataset = AlzheimerDataset(file_path)\n",
    "\n",
    "train_data, test_data = dataset.random_split(test_ratio=0.2, random_state=0)\n",
    "\n",
    "# Define input variables (X) and target variable (Y)\n",
    "X = train_data.numericals[['MMSE', 'FunctionalAssessment']].to_numpy()\n",
    "Y = train_data.numericals['ADL'].to_numpy()\n",
    "\n",
    "# Define the models\n",
    "linear_model = LinearRegression()\n",
    "ridge_model = Ridge(alpha=1000)  # Use alpha=1.0 as a starting point for Ridge\n",
    "\n",
    "# Define RMSE as the scoring metric\n",
    "rmse_scorer = make_scorer(mean_squared_error, squared=False)\n",
    "\n",
    "# Perform 5-fold cross-validation for Linear Regression\n",
    "linear_rmse_scores = cross_val_score(linear_model, X, Y, cv=5, scoring=rmse_scorer)\n",
    "linear_avg_rmse = np.mean(linear_rmse_scores)\n",
    "print(f\"Linear Regression - Average RMSE: {linear_avg_rmse:.4f}\")\n",
    "\n",
    "# Perform 5-fold cross-validation for Ridge Regression\n",
    "ridge_rmse_scores = cross_val_score(ridge_model, X, Y, cv=5, scoring=rmse_scorer)\n",
    "ridge_avg_rmse = np.mean(ridge_rmse_scores)\n",
    "print(f\"Ridge Regression - Average RMSE: {ridge_avg_rmse:.4f}\")\n",
    "\n",
    "# Compare the results\n",
    "if linear_avg_rmse < ridge_avg_rmse:\n",
    "    print(\"Linear Regression performs better on average.\")\n",
    "elif ridge_avg_rmse < linear_avg_rmse:\n",
    "    print(\"Ridge Regression performs better on average.\")\n",
    "else:\n",
    "    print(\"Both models perform equally well.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Coefficients: [0.07668555 0.14679183]\n",
      "Intercept: 2.3707350510966405\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from alzheimer import AlzheimerDataset\n",
    "import pathlib\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "# Définir _CWD comme le répertoire de travail actuel\n",
    "_CWD = pathlib.Path(os.getcwd())\n",
    "\n",
    "# Load and preprocess the data\n",
    "file_path = r\"C:\\Users\\andre\\Downloads\\Data-20241216\\alzheimers_disease_data.csv\"\n",
    "dataset = AlzheimerDataset(file_path)\n",
    "\n",
    "train_data, _ = dataset.random_split(test_ratio=0.2, random_state=0)\n",
    "\n",
    "X = train_data.numericals[['MMSE', 'FunctionalAssessment']].to_numpy()\n",
    "Y = train_data.numericals['ADL'].to_numpy()\n",
    "\n",
    "# Train the Linear Regression model\n",
    "linear_reg = LinearRegression()\n",
    "linear_reg.fit(X, Y)\n",
    "\n",
    "# Extract coefficients and intercept\n",
    "coefficients = linear_reg.coef_\n",
    "intercept = linear_reg.intercept_\n",
    "\n",
    "print(\"Linear Regression Coefficients:\", coefficients)\n",
    "print(\"Intercept:\", intercept)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression RMSE: 2.9742876159594687\n",
      "Ridge Regression RMSE: 2.9742855433479294\n",
      "Wilcoxon signed-rank test statistic: 44501.0\n",
      "p-value: 0.5292815148639671\n",
      "There is no significant difference between Linear Regression and Ridge Regression results.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import wilcoxon\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Prepare the data (Assuming train_data and test_data are already defined)\n",
    "X_train, Y_train = get_selected_vars(train_data, X_vars=['MMSE', 'FunctionalAssessment'], Y_var='ADL')\n",
    "X_test, Y_test = get_selected_vars(test_data, X_vars=['MMSE', 'FunctionalAssessment'], Y_var='ADL')\n",
    "\n",
    "# Step 2: Train the models\n",
    "linear_model = LinearRegression()\n",
    "ridge_model = Ridge(alpha=1.0)\n",
    "\n",
    "linear_model.fit(X_train, Y_train)\n",
    "ridge_model.fit(X_train, Y_train)\n",
    "\n",
    "# Step 3: Predict on the test set\n",
    "linear_preds = linear_model.predict(X_test)\n",
    "ridge_preds = ridge_model.predict(X_test)\n",
    "\n",
    "# Step 4: Compute RMSE for both models\n",
    "linear_rmse = np.sqrt(mean_squared_error(Y_test, linear_preds))\n",
    "ridge_rmse = np.sqrt(mean_squared_error(Y_test, ridge_preds))\n",
    "\n",
    "print(f\"Linear Regression RMSE: {linear_rmse}\")\n",
    "print(f\"Ridge Regression RMSE: {ridge_rmse}\")\n",
    "\n",
    "# Step 5: Perform the Wilcoxon signed-rank test\n",
    "# Compare RMSE on each test sample (squared errors)\n",
    "linear_errors = (Y_test - linear_preds) ** 2\n",
    "ridge_errors = (Y_test - ridge_preds) ** 2\n",
    "\n",
    "stat, p_value = wilcoxon(linear_errors, ridge_errors)\n",
    "print(f\"Wilcoxon signed-rank test statistic: {stat}\")\n",
    "print(f\"p-value: {p_value}\")\n",
    "\n",
    "# Step 6: Interpretation\n",
    "if p_value < 0.05:\n",
    "    print(\"There is a significant difference between Linear Regression and Ridge Regression results.\")\n",
    "else:\n",
    "    print(\"There is no significant difference between Linear Regression and Ridge Regression results.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse statistique du code Python\n",
    "\n",
    "Ce code met en œuvre plusieurs techniques de régression (Régression Linéaire et Régression Ridge) pour modéliser une relation entre des variables prédictives (`MMSE` et `FunctionalAssessment`) et une variable cible (`ADL`) à partir d’un jeu de données sur la maladie d'Alzheimer. Voici une analyse approfondie :\n",
    "\n",
    "---\n",
    "\n",
    "#### **1. Objectifs principaux**\n",
    "Le code vise à :\n",
    "1. **Modéliser la relation entre les variables prédictives et la variable cible** à l’aide de modèles de régression linéaire.\n",
    "2. **Évaluer les performances des modèles** en utilisant des métriques telles que l'erreur quadratique moyenne (RMSE) et le coefficient de détermination (\\(R^2\\)).\n",
    "3. **Comparer la Régression Linéaire et la Régression Ridge** en termes de performance moyenne via la validation croisée.\n",
    "4. **Tester la significativité des différences de performance** entre ces modèles à l’aide du test de Wilcoxon.\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Méthodes statistiques**\n",
    "\n",
    "##### **Régression Linéaire**\n",
    "- La Régression Linéaire estime une relation linéaire entre les variables prédictives (\\(X\\)) et la variable cible (\\(Y\\)).\n",
    "- Les coefficients (\\(\\beta\\)) et l'intercept (\\(\\beta_0\\)) indiquent l'ampleur et la direction de l'effet de chaque variable prédictive sur la variable cible.\n",
    "\n",
    "##### **Régression Ridge**\n",
    "- Cette variante pénalise la taille des coefficients via une régularisation L2 (\\(\\alpha ||\\beta||^2\\)), ce qui réduit le risque de surapprentissage, surtout en présence de multicolinéarité ou de nombreuses variables prédictives.\n",
    "\n",
    "##### **Validation croisée**\n",
    "- La validation croisée à 5 plis partitionne les données pour évaluer la performance moyenne du modèle tout en minimisant les biais dus à une seule division des données.\n",
    "\n",
    "##### **Métriques d'évaluation**\n",
    "- **RMSE (Root Mean Squared Error)** : Mesure la moyenne des écarts quadratiques entre les valeurs observées et prédites.\n",
    "  \\[\n",
    "  RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2}\n",
    "  \\]\n",
    "  Un RMSE plus faible indique une meilleure performance.\n",
    "  \n",
    "- **\\(R^2\\) (Coefficient de détermination)** : Évalue la proportion de variance expliquée par le modèle.\n",
    "  \\[\n",
    "  R^2 = 1 - \\frac{\\text{SS}_{\\text{residual}}}{\\text{SS}_{\\text{total}}}\n",
    "  \\]\n",
    "  Une valeur proche de 1 indique une forte capacité explicative.\n",
    "\n",
    "##### **Test de Wilcoxon**\n",
    "- Le test de Wilcoxon est utilisé pour comparer les erreurs quadratiques des deux modèles sur chaque échantillon de test.\n",
    "- Hypothèses :\n",
    "  - \\(H_0\\) : Les erreurs des deux modèles suivent une distribution identique.\n",
    "  - \\(H_1\\) : Il existe une différence significative dans les erreurs entre les deux modèles.\n",
    "- Une \\(p\\)-value inférieure à 0.05 indique une différence significative.\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Résultats attendus**\n",
    "1. **Régression Linéaire**\n",
    "   - Coefficients interprétables montrant comment les prédicteurs affectent directement `ADL`.\n",
    "   - Performances évaluées par RMSE et \\(R^2\\). Si le \\(R^2\\) est faible, cela peut indiquer que le modèle ne capture pas bien les relations.\n",
    "\n",
    "2. **Régression Ridge**\n",
    "   - Réduction du surajustement grâce à la pénalisation L2.\n",
    "   - Performance légèrement meilleure ou comparable à celle de la Régression Linéaire si les données ne sont pas bruitées.\n",
    "\n",
    "3. **Validation croisée**\n",
    "   - RMSE moyen des deux modèles pour évaluer la robustesse des résultats.\n",
    "   - Une réduction du RMSE dans Ridge indique que la régularisation aide à éviter le surajustement.\n",
    "\n",
    "4. **Test de Wilcoxon**\n",
    "   - Si \\(p < 0.05\\), Ridge est statistiquement supérieur à la Régression Linéaire.\n",
    "   - Sinon, les deux modèles sont considérés comme équivalents en termes de performance.\n",
    "\n",
    "---\n",
    "\n",
    "#### **4. Points forts et limites**\n",
    "\n",
    "##### **Points forts**\n",
    "- **Sélection de variables pertinentes** : Seules deux variables (`MMSE` et `FunctionalAssessment`) sont utilisées, ce qui réduit le risque d’inclure du bruit inutile.\n",
    "- **Comparaison robuste** : Validation croisée et test statistique garantissent une évaluation fiable des modèles.\n",
    "- **Interprétabilité** : Les coefficients de la Régression Linéaire permettent de comprendre directement l'effet des prédicteurs.\n",
    "\n",
    "##### **Limites**\n",
    "- **Données possiblement bruitées** : La simple Régression Linéaire peut être affectée par des relations non linéaires ou des données bruitées.\n",
    "- **Hypothèses linéaires** : Les relations entre les variables ne sont pas vérifiées, ce qui peut limiter la pertinence des modèles.\n",
    "- **Taille des données** : Si le jeu de données est petit, la validation croisée peut produire des estimations moins fiables.\n",
    "\n",
    "---\n",
    "\n",
    "#### **5. Conclusion statistique**\n",
    "- La Régression Ridge est attendue pour offrir une meilleure robustesse, surtout si les prédicteurs sont corrélés ou si le nombre d'échantillons est limité.\n",
    "- La comparaison via validation croisée et test de Wilcoxon garantit une évaluation objective des modèles.\n",
    "- Une exploration supplémentaire, comme l'examen de la distribution des résidus ou des relations non linéaires, pourrait affiner davantage les conclusions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
